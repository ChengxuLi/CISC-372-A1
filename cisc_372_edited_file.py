# -*- coding: utf-8 -*-
"""CISC 372 edited file.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12zjISwXvsEfUirwSCCVCiCk5HBgdBBMP
"""

# download data (-q is the quiet mode)
! wget -q https://www.dropbox.com/s/lhb1awpi769bfdr/test.csv?dl=1 -O test.csv
! wget -q https://www.dropbox.com/s/gudb5eunj700s7j/train.csv?dl=1 -O train.csv
# downloading the data as testing and training data sets
import pandas as pd
# imports a module / library called pandas and name is as pd in the following code
Xy_train = pd.read_csv('train.csv', engine='python')
# read the train.csv file to get comma-seperated values using python
X_train = Xy_train.drop(columns=['price_rating'])
# drop the column "price-rating" since it is the class label
y_train = Xy_train[['price_rating']]
# put the value in variable "y_train"

print('traning', len(X_train))
Xy_train.price_rating.hist()
# get a histogram by price_rating values for the training data

X_test = pd.read_csv('test.csv', engine='python')
testing_ids = X_test.Id
print('testing', len(X_test))
# get testing values

# model training and tuning
import numpy as np
from sklearn.compose import ColumnTransformer
from sklearn.datasets import fetch_openml
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from xgboost.sklearn import XGBClassifier

np.random.seed(0)
# make the random numbers predictable
numeric_features = ['bedrooms', 'review_scores_location', 'accommodates', 'beds','bathrooms','review_scores_value','review_scores_cleanliness','review_scores_rating']
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())])
# selects numeric features to be considered in the XGBclassifier
# I added several features into numeric features using common sense. (I travel a lot, so I'm putting in all the features that I consider important when I'm booking)
# For example: 'bathrooms' is always an important thing to look at, because that decides how many people can take shower at a time. And sometimes, girls and boys would like to use different bathroom for privacy.
categorical_features = [
  'property_type', 'is_business_travel_ready', 'room_type','cancellation_policy','host_is_superhost']
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])
# For example: I chose "superhost" as one of the features to look at because it's a tag offered by airbnb, meaning airbnb has already considered if the host is responsible and available for replying messages.
# Additionally, it's computationally cheap because it's a binary choice, t and f.
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)])

regr = Pipeline(steps=[('preprocessor', preprocessor),
                      ('regressor', XGBClassifier(
                          objective='multi:softmax', seed=1))])
# concatenates the columns and respective transformer into one transformer as preprocessor
# prepare to do a regression prediction by XGB classifier, name it by regressor

X_train = X_train[[*numeric_features, *categorical_features]]
X_test = X_test[[*numeric_features, *categorical_features]]

# `__` denotes attribute 
# (e.g. regressor__n_estimators means the `n_estimators` param for `regressor`
#  which is our xgb)
param_grid = {
    'preprocessor__num__imputer__strategy': ['mean'],
    'regressor__n_estimators': [225],
    'regressor__max_depth':[3]
}
# configures the XGBclassifier, number of trees, max depth...
# Here I changed the regressor_n_estimators to 100-200 because after how I modified the features, the number of features are almost doubled. Therefore I doubled the estimators as well. 
# There are 13 features in total, 13^2 = 169, this is another reason why I changed it to 100-200
grid_search = GridSearchCV(
    regr, param_grid, cv=10, verbose=3, n_jobs=-1, 
    scoring='accuracy')
grid_search.fit(X_train, y_train)
# configures the grid_search_cross_validation settings
# Here I changed the n_jobs to -1 to use all processors available to speed up the running process
# Also, I changed the cv to 8 because we are using XGBclassifier and I didn't use any feature like "Host name" or "Host ID".. So I'm not too worried about overfitting, instead I want to improve the accuracy.
print('best score {}'.format(grid_search.best_score_))
# prints out the best score found by grid_search
# Prediction & generating the submission file
y_pred = grid_search.predict(X_test)
pd.DataFrame({'Id': testing_ids, 'price_rating':y_pred}).to_csv('sample_submission.csv', index=False)
# constructs the sample_submission.csv





